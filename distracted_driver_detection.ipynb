{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "random_state = 424\n",
    "dataset_path = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集是26位司机的各种样子的照片。所以我按照司机来划分训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取csv文件\n",
    "dataset = pd.read_csv('dataset/driver_imgs_list.csv')\n",
    "\n",
    "# 将前21为司机的照片作为训练集，后2位司机的照片作为测试集\n",
    "drive_id_train = dataset['subject'].unique()[:-2]\n",
    "drive_id_test = dataset['subject'].unique()[-2:]\n",
    "\n",
    "# 组合路径\n",
    "dataset['path']=dataset[['classname', 'img']].apply(lambda x: '/'.join(x), axis=1)\n",
    "\n",
    "# 将路径转换成list格式\n",
    "train_path = dataset['path'].loc[dataset['subject'].isin(drive_id_train)].tolist()\n",
    "test_path = dataset['path'].loc[dataset['subject'].isin(drive_id_test)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20787 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset copy begain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20787/20787 [06:06<00:00, 56.70it/s] \n",
      "  0%|          | 0/1637 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset copy begain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637/1637 [00:02<00:00, 612.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "if os.path.exists('dataset/train2'):\n",
    "    print('split dataset exist')\n",
    "else:\n",
    "    os.mkdir('dataset/train2')\n",
    "    os.mkdir('dataset/valid2')\n",
    "    for i in range(10):\n",
    "        os.mkdir('dataset/train2/c'+str(i))\n",
    "        os.mkdir('dataset/valid2/c'+str(i))\n",
    "    # 复制图片到新的数据集\n",
    "    if os.path.exists('dataset/train2'):\n",
    "        print('train dataset copy begain')\n",
    "        for filename in tqdm(train_path):\n",
    "            shutil.copyfile('dataset/train/'+filename, 'dataset/train2/'+filename)\n",
    "    if os.path.exists('dataset/valid2'):\n",
    "        print('test dataset copy begain')\n",
    "        for filename in tqdm(test_path):\n",
    "            shutil.copyfile('dataset/train/'+filename, 'dataset/valid2/'+filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练模型模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.applications import *\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_dir = 'dataset/train2'  # 训练集数据\n",
    "val_dir = 'dataset/valid2' # 验证集数据\n",
    "nb_classes = len(glob.glob(train_dir + \"/*\"))  # 分类数\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "# def VGG16_model(X_train, y_train, X_test, y_test):\n",
    "def run_a_model(BASE_MODEL,input_shape, fine_tune_layer, preprocessing=None, model_name_option=''):\n",
    "    input_height = input_shape[0]\n",
    "    input_width = input_shape[1]\n",
    "    input_tensor = Input((input_height, input_width, 3))\n",
    "    base_model = BASE_MODEL(input_tensor=Input((input_height, input_width, 3)),\n",
    "                            weights='imagenet', \n",
    "                            include_top=False, \n",
    "                            input_shape=(input_height, input_width, 3)\n",
    "                           )\n",
    "    \n",
    "    \n",
    "    x = input_tensor\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(base_model.input, x)    \n",
    "    print(\"total layer count {}\".format(len(base_model.layers)))\n",
    "    \n",
    "    for i in range(fine_tune_layer):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        preprocessing_function=preprocessing\n",
    "    )\n",
    "    \n",
    "    test_gen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing\n",
    "    )\n",
    "    \n",
    "    train_generator = train_gen.flow_from_directory(train_dir, (input_height, input_width), \n",
    "                                                    shuffle=True, batch_size=batch_size, class_mode='categorical')\n",
    "    test_generator = test_gen.flow_from_directory(val_dir, (input_height, input_width), \n",
    "                                                  shuffle=True, batch_size=batch_size, class_mode='categorical')\n",
    "    \n",
    "    \n",
    "    steps_train_sample = train_generator.samples // batch_size + 1\n",
    "    steps_valid_sample = test_generator.samples // batch_size + 1\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_train_sample,\n",
    "        epochs=5,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=steps_valid_sample)\n",
    "    '''\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_train_sample,\n",
    "        epochs=5,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=steps_valid_sample)\n",
    "    '''\n",
    "    \n",
    "    model.save(\"models/model_{}{}.h5\".format(BASE_MODEL.func_name, model_name_option))\n",
    "\n",
    "    return\n",
    "\n",
    "def model_optimizer(model_name,BASE_MODEL, input_shape, fine_tune_layer, preprocessing=None, model_name_option=''):    \n",
    "    input_height = input_shape[0]\n",
    "    input_width = input_shape[1]\n",
    "    base_model = BASE_MODEL(input_tensor=Input((input_height, input_width, 3)),\n",
    "                            weights=None, \n",
    "                            include_top=False, \n",
    "                            input_shape=(input_height, input_width, 3)\n",
    "                           )  \n",
    "    \n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    model.load_weights(\"models/\"+model_name , by_name=True)\n",
    "    for i in range(fine_tune_layer):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        preprocessing_function=preprocessing\n",
    "    )\n",
    "    \n",
    "    test_gen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing\n",
    "    )\n",
    "    \n",
    "    train_generator = train_gen.flow_from_directory(train_dir, (input_height, input_width), \n",
    "                                                    shuffle=True, batch_size=batch_size, class_mode='categorical')\n",
    "    test_generator = test_gen.flow_from_directory(val_dir, (input_height, input_width), \n",
    "                                                  shuffle=True, batch_size=batch_size, class_mode='categorical')\n",
    "    \n",
    "    \n",
    "    steps_train_sample = train_generator.samples // batch_size + 1\n",
    "    steps_valid_sample = test_generator.samples // batch_size + 1\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_train_sample,\n",
    "        epochs=5,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=steps_valid_sample)\n",
    "    \n",
    "    \n",
    "    model.save(\"models/model_optimized_{}{}_.h5\".format(BASE_MODEL.func_name, model_name_option))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 175\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 324s - loss: 0.2577 - acc: 0.9191 - val_loss: 2.6435 - val_acc: 0.5504\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 265s - loss: 0.0574 - acc: 0.9825 - val_loss: 1.3095 - val_acc: 0.7495\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 267s - loss: 0.0394 - acc: 0.9873 - val_loss: 1.1674 - val_acc: 0.7337\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 266s - loss: 0.0348 - acc: 0.9893 - val_loss: 1.2046 - val_acc: 0.7385\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 266s - loss: 0.0239 - acc: 0.9925 - val_loss: 1.0548 - val_acc: 0.7428\n"
     ]
    }
   ],
   "source": [
    "# ResNet50\n",
    "run_a_model(ResNet50, (224, 224), 152, optimizer='adam', preprocessing=None, model_name_option='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 311\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 509s - loss: 0.1854 - acc: 0.9427 - val_loss: 1.2199 - val_acc: 0.7575\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 493s - loss: 0.0370 - acc: 0.9897 - val_loss: 1.0980 - val_acc: 0.7764\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 493s - loss: 0.0214 - acc: 0.9945 - val_loss: 0.3628 - val_acc: 0.8968\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 493s - loss: 0.0192 - acc: 0.9952 - val_loss: 0.8447 - val_acc: 0.8155\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 493s - loss: 0.0202 - acc: 0.9940 - val_loss: 1.1769 - val_acc: 0.7795\n"
     ]
    }
   ],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 172, optimizer='adam', preprocessing=inception_v3.preprocess_input, model_name_option='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 132\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 627s - loss: 0.2350 - acc: 0.9277 - val_loss: 0.8607 - val_acc: 0.7019\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 621s - loss: 0.0284 - acc: 0.9912 - val_loss: 3.8232 - val_acc: 0.3519\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 621s - loss: 0.0155 - acc: 0.9958 - val_loss: 0.7173 - val_acc: 0.8296\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 621s - loss: 0.0112 - acc: 0.9966 - val_loss: 0.8089 - val_acc: 0.7764\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 621s - loss: 0.0134 - acc: 0.9962 - val_loss: 0.5681 - val_acc: 0.8363\n"
     ]
    }
   ],
   "source": [
    "run_a_model(Xception, (299, 299), 116, optimizer='adam', preprocessing=xception.preprocess_input, model_name_option='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 175\n",
      "Found 21601 images belonging to 10 classes.\n",
      "Found 823 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "169/169 [==============================] - 438s - loss: 0.2612 - acc: 0.9152 - val_loss: 1.2752 - val_acc: 0.6537\n",
      "Epoch 2/5\n",
      "169/169 [==============================] - 430s - loss: 0.0590 - acc: 0.9824 - val_loss: 0.7990 - val_acc: 0.8433\n",
      "Epoch 3/5\n",
      "169/169 [==============================] - 430s - loss: 0.0420 - acc: 0.9864 - val_loss: 1.2145 - val_acc: 0.7582\n",
      "Epoch 4/5\n",
      "169/169 [==============================] - 430s - loss: 0.0254 - acc: 0.9927 - val_loss: 1.4943 - val_acc: 0.6574\n",
      "Epoch 5/5\n",
      "169/169 [==============================] - 430s - loss: 0.0282 - acc: 0.9916 - val_loss: 0.9556 - val_acc: 0.7327\n",
      "Epoch 1/5\n",
      "169/169 [==============================] - 434s - loss: 0.0115 - acc: 0.9970 - val_loss: 0.7865 - val_acc: 0.7436\n",
      "Epoch 2/5\n",
      "169/169 [==============================] - 431s - loss: 0.0076 - acc: 0.9980 - val_loss: 0.6451 - val_acc: 0.7716\n",
      "Epoch 3/5\n",
      "169/169 [==============================] - 430s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.7206 - val_acc: 0.7570\n",
      "Epoch 4/5\n",
      "169/169 [==============================] - 430s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.8070 - val_acc: 0.7388\n",
      "Epoch 5/5\n",
      "169/169 [==============================] - 430s - loss: 0.0050 - acc: 0.9988 - val_loss: 0.6742 - val_acc: 0.7691\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 156 finetune Layer 10 epoches\n",
    "run_a_model(ResNet50, (240, 360), 154, preprocessing=None, model_name_option='_154')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 175\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 282s - loss: 0.3123 - acc: 0.9097 - val_loss: 1.7951 - val_acc: 0.4924\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 262s - loss: 0.0667 - acc: 0.9799 - val_loss: 1.6361 - val_acc: 0.6151\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 262s - loss: 0.0466 - acc: 0.9845 - val_loss: 2.0096 - val_acc: 0.5748\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 262s - loss: 0.0321 - acc: 0.9895 - val_loss: 1.3769 - val_acc: 0.6811\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 262s - loss: 0.0226 - acc: 0.9918 - val_loss: 1.4909 - val_acc: 0.6946\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 277s - loss: 0.0151 - acc: 0.9948 - val_loss: 1.2312 - val_acc: 0.7141\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 262s - loss: 0.0103 - acc: 0.9964 - val_loss: 1.1701 - val_acc: 0.7147\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 262s - loss: 0.0091 - acc: 0.9969 - val_loss: 1.0920 - val_acc: 0.7300\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 262s - loss: 0.0078 - acc: 0.9976 - val_loss: 1.1681 - val_acc: 0.7184\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 263s - loss: 0.0086 - acc: 0.9974 - val_loss: 1.0678 - val_acc: 0.7330\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 154 finetune layer with BN 10 epoches\n",
    "run_a_model(ResNet50, (224, 224), 160, preprocessing=None, model_name_option='_BN_160')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_a_model(ResNet50, (224, 224), 165, preprocessing=None, model_name_option='_BN_165')\n",
    "# 两代 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 19\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 369s - loss: 0.2424 - acc: 0.9305 - val_loss: 3.5156 - val_acc: 0.4374\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 343s - loss: 0.0398 - acc: 0.9882 - val_loss: 2.2552 - val_acc: 0.5803\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 344s - loss: 0.0192 - acc: 0.9952 - val_loss: 2.3463 - val_acc: 0.6897\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 344s - loss: 0.0132 - acc: 0.9962 - val_loss: 2.1964 - val_acc: 0.5791\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 344s - loss: 0.0108 - acc: 0.9972 - val_loss: 1.0301 - val_acc: 0.7703\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 347s - loss: 0.0076 - acc: 0.9985 - val_loss: 1.0063 - val_acc: 0.7385\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 343s - loss: 0.0034 - acc: 0.9992 - val_loss: 1.0395 - val_acc: 0.7367\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 344s - loss: 0.0020 - acc: 0.9996 - val_loss: 0.9318 - val_acc: 0.7550\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 343s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.9079 - val_acc: 0.7831\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 343s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.9276 - val_acc: 0.7838\n"
     ]
    }
   ],
   "source": [
    "# vgg16 with BN,finetune layer 15  10 epoche\n",
    "run_a_model(VGG16, (224, 224), 15, preprocessing=None, model_name_option='_BN_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 311\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 506s - loss: 0.2324 - acc: 0.9389 - val_loss: 0.6160 - val_acc: 0.8082\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 489s - loss: 0.0385 - acc: 0.9894 - val_loss: 0.8902 - val_acc: 0.7666\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 489s - loss: 0.0316 - acc: 0.9911 - val_loss: 0.8109 - val_acc: 0.8302\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 489s - loss: 0.0157 - acc: 0.9951 - val_loss: 0.6184 - val_acc: 0.8552\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 489s - loss: 0.0148 - acc: 0.9956 - val_loss: 1.3184 - val_acc: 0.7166\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 497s - loss: 0.0075 - acc: 0.9975 - val_loss: 0.9058 - val_acc: 0.7954\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 489s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.8289 - val_acc: 0.8216\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 489s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.8779 - val_acc: 0.8277\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 489s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.8738 - val_acc: 0.8357\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 489s - loss: 0.0026 - acc: 0.9994 - val_loss: 0.9565 - val_acc: 0.8265\n"
     ]
    }
   ],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 180, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_180')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 311\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 472s - loss: 0.2829 - acc: 0.9258 - val_loss: 0.8754 - val_acc: 0.7502\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 464s - loss: 0.0513 - acc: 0.9857 - val_loss: 0.5692 - val_acc: 0.8180\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 464s - loss: 0.0228 - acc: 0.9929 - val_loss: 1.3623 - val_acc: 0.7434\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 464s - loss: 0.0163 - acc: 0.9950 - val_loss: 0.8817 - val_acc: 0.8088\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 464s - loss: 0.0156 - acc: 0.9952 - val_loss: 0.3770 - val_acc: 0.9023\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 473s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.4018 - val_acc: 0.8821\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 464s - loss: 0.0043 - acc: 0.9987 - val_loss: 0.5527 - val_acc: 0.8503\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 465s - loss: 0.0027 - acc: 0.9992 - val_loss: 0.5721 - val_acc: 0.8509\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 465s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.6307 - val_acc: 0.8454\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 465s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.6199 - val_acc: 0.8448\n"
     ]
    }
   ],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 200, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优秀模型继续优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('models/model_InceptionV3_BN_200.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img(path, image_shape):\n",
    "    img = cv2.imread(path, 0)\n",
    "    resized = cv2.resize(img, image_shape)\n",
    "    return resized\n",
    "\n",
    "def load_test(img_rows, img_cols):\n",
    "    print('Read test images')\n",
    "    path = os.path.join('dataset', 'to_prediction', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    for fl in tqdm(files):\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_img(fl, (img_rows, img_cols))\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "    \n",
    "    return np.divide(X_test,255.0) , X_test_id\n",
    "\n",
    "def KNN(input_data, predictions, N=5, coef = [0.05, 0.1, 0.15, 0.2, 0.5]):\n",
    "    new_predictions_list = []\n",
    "    for i in tqdm(range(len(input_data))):\n",
    "        distance = np.square(np.subtract(input_data, input_data[i]))\n",
    "        distance = np.sqrt(np.sum(distance, axis=(1, 2)))\n",
    "        N_min_index = np.argsort(distance)[::-1][-N:]\n",
    "        \n",
    "        pred = [ predictions[N_min_index[j]] * coef[j] for j in range(N)]\n",
    "        pred = np.sum(pred, axis=0)\n",
    "        new_predictions_list.append(pred)\n",
    "                \n",
    "    return np.array(new_predictions_list)\n",
    "\n",
    "def make_predictions(MODEL, image_size, batch_size, preprocessing=None):\n",
    "    gen = ImageDataGenerator(preprocessing_function=preprocessing)\n",
    "    path_test_data = 'dataset/to_prediction'\n",
    "    test_generator = gen.flow_from_directory(path_test_data,  image_size, shuffle=False, \n",
    "                                             batch_size=batch_size, class_mode='categorical')\n",
    "    y_predictions = MODEL.predict_generator(test_generator,  steps=test_generator.samples//batch_size+1,  verbose=1)\n",
    "    # y_predictions = y_predictions.clip(min=0.005, max=0.995)\n",
    "    \n",
    "    test_id = list()\n",
    "    for i, file_name in enumerate(test_generator.filenames):\n",
    "        flbase = os.path.basename(file_name)\n",
    "        test_id.append(flbase)       \n",
    "    \n",
    "    return y_predictions, test_id\n",
    "\n",
    "def create_submission(predictions, test_id):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3',\n",
    "                                                 'c4', 'c5', 'c6', 'c7',\n",
    "                                                 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n",
      "623/623 [==============================] - 1960s  \n"
     ]
    }
   ],
   "source": [
    "y_predictions, test_id = make_predictions(model, (299, 299), 128, preprocessing=inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/79726 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read test images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79726/79726 [02:10<00:00, 609.74it/s]\n"
     ]
    }
   ],
   "source": [
    "input_data, img_id = load_test(40, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 116/79726 [01:44<19:43:11,  1.12it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-385622f873bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.995\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-7221ad3cd632>\u001b[0m in \u001b[0;36mKNN\u001b[0;34m(input_data, predictions, N, coef)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnew_predictions_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mN_min_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_predictions = KNN(input_data, y_predictions)\n",
    "new_predictions = new_predictions.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(y_predictions, test_id)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlnd-dl]",
   "language": "python",
   "name": "conda-env-mlnd-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
