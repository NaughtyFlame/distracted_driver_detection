{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "random_state = 424\n",
    "dataset_path = 'dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集是26位司机的各种样子的照片。所以我按照司机来划分训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p002' 'p047']\n"
     ]
    }
   ],
   "source": [
    "# 读取csv文件\n",
    "dataset = pd.read_csv('dataset/driver_imgs_list.csv')\n",
    "\n",
    "# 随机选择两个司机作为测试集\n",
    "driver_id = dataset['subject'].unique()\n",
    "driver_id_test = np.random.choice(driver_id, 2)\n",
    "print(driver_id_test)\n",
    "\n",
    "# 组合路径\n",
    "dataset['path']=dataset[['classname', 'img']].apply(lambda x: '/'.join(x), axis=1)\n",
    "\n",
    "# 将路径转换成list格式\n",
    "train_path = dataset['path'].loc[~dataset['subject'].isin(driver_id_test)].tolist()\n",
    "test_path = dataset['path'].loc[dataset['subject'].isin(driver_id_test)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20864 [00:00<1:06:08,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset copy begain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20864/20864 [05:38<00:00, 61.60it/s] \n",
      "  5%|▌         | 82/1560 [00:00<00:01, 818.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset copy begain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:01<00:00, 925.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "if os.path.exists('dataset/train2'):\n",
    "    print('split dataset exist')\n",
    "else:\n",
    "    os.mkdir('dataset/train2')\n",
    "    os.mkdir('dataset/valid2')\n",
    "    for i in range(10):\n",
    "        os.mkdir('dataset/train2/c'+str(i))\n",
    "        os.mkdir('dataset/valid2/c'+str(i))\n",
    "    # 复制图片到新的数据集\n",
    "    if os.path.exists('dataset/train2'):\n",
    "        print('train dataset copy begain')\n",
    "        for filename in tqdm(train_path):\n",
    "            shutil.copyfile('dataset/train/'+filename, 'dataset/train2/'+filename)\n",
    "    if os.path.exists('dataset/valid2'):\n",
    "        print('test dataset copy begain')\n",
    "        for filename in tqdm(test_path):\n",
    "            shutil.copyfile('dataset/train/'+filename, 'dataset/valid2/'+filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练模型模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.applications import *\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_dir = 'dataset/train2'  # 训练集数据\n",
    "val_dir = 'dataset/valid2' # 验证集数据\n",
    "nb_classes = len(glob.glob(train_dir + \"/*\"))  # 分类数\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "# def VGG16_model(X_train, y_train, X_test, y_test):\n",
    "def run_a_model(BASE_MODEL,input_shape, fine_tune_layer, preprocessing=None, model_name_option=''):\n",
    "    input_height = input_shape[0]\n",
    "    input_width = input_shape[1]\n",
    "    input_tensor = Input((input_height, input_width, 3))\n",
    "    base_model = BASE_MODEL(input_tensor=Input((input_height, input_width, 3)),\n",
    "                            weights='imagenet', \n",
    "                            include_top=False, \n",
    "                            input_shape=(input_height, input_width, 3)\n",
    "                           )\n",
    "    \n",
    "    \n",
    "    x = input_tensor\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(base_model.input, x)    \n",
    "    print(\"total layer count {}\".format(len(base_model.layers)))\n",
    "    \n",
    "    for i in range(fine_tune_layer):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    train_gen = ImageDataGenerator(\n",
    "        rotation_range=10.,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        preprocessing_function=preprocessing\n",
    "    )\n",
    "    \n",
    "    test_gen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing\n",
    "    )\n",
    "    \n",
    "    train_generator = train_gen.flow_from_directory(train_dir, (input_height, input_width), \n",
    "                                                    shuffle=True, batch_size=batch_size, class_mode='categorical')\n",
    "    test_generator = test_gen.flow_from_directory(val_dir, (input_height, input_width), \n",
    "                                                  shuffle=True, batch_size=batch_size, class_mode='categorical')\n",
    "    \n",
    "    \n",
    "    steps_train_sample = train_generator.samples // batch_size + 1\n",
    "    steps_valid_sample = test_generator.samples // batch_size + 1\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_train_sample,\n",
    "        epochs=5,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=steps_valid_sample)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_train_sample,\n",
    "        epochs=5,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=steps_valid_sample)    \n",
    "    \n",
    "    model.save(\"models/model_{}{}.h5\".format(BASE_MODEL.func_name, model_name_option))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_loss_plot(model):\n",
    "    # list all data in history\n",
    "    print(model.history.keys())\n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 15 fine tune layer \n",
    "vgg16 = run_a_model(VGG16, (224, 224), 15, model_name_option='_BN_15_SGD_10epoche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet50 155 finetune layer with BN 10 epoches\n",
    "resnet50 = run_a_model(ResNet50, (224, 224), 155, model_name_option='_BN_155_SGD_10epoche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# InceptionV3 200 finetune layer with BN 10 epoches\n",
    "inceptionv3 = run_a_model(InceptionV3, (299, 299), 200, \n",
    "                          preprocessing=inception_v3.preprocess_input, model_name_option='_BN_200_SGD_10epoche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的舍弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 175\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 282s - loss: 0.3123 - acc: 0.9097 - val_loss: 1.7951 - val_acc: 0.4924\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 262s - loss: 0.0667 - acc: 0.9799 - val_loss: 1.6361 - val_acc: 0.6151\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 262s - loss: 0.0466 - acc: 0.9845 - val_loss: 2.0096 - val_acc: 0.5748\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 262s - loss: 0.0321 - acc: 0.9895 - val_loss: 1.3769 - val_acc: 0.6811\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 262s - loss: 0.0226 - acc: 0.9918 - val_loss: 1.4909 - val_acc: 0.6946\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 277s - loss: 0.0151 - acc: 0.9948 - val_loss: 1.2312 - val_acc: 0.7141\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 262s - loss: 0.0103 - acc: 0.9964 - val_loss: 1.1701 - val_acc: 0.7147\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 262s - loss: 0.0091 - acc: 0.9969 - val_loss: 1.0920 - val_acc: 0.7300\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 262s - loss: 0.0078 - acc: 0.9976 - val_loss: 1.1681 - val_acc: 0.7184\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 263s - loss: 0.0086 - acc: 0.9974 - val_loss: 1.0678 - val_acc: 0.7330\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 154 finetune layer with BN 10 epoches\n",
    "run_a_model(ResNet50, (224, 224), 160, preprocessing=None, model_name_option='_BN_160')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 19\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 369s - loss: 0.2424 - acc: 0.9305 - val_loss: 3.5156 - val_acc: 0.4374\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 343s - loss: 0.0398 - acc: 0.9882 - val_loss: 2.2552 - val_acc: 0.5803\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 344s - loss: 0.0192 - acc: 0.9952 - val_loss: 2.3463 - val_acc: 0.6897\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 344s - loss: 0.0132 - acc: 0.9962 - val_loss: 2.1964 - val_acc: 0.5791\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 344s - loss: 0.0108 - acc: 0.9972 - val_loss: 1.0301 - val_acc: 0.7703\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 347s - loss: 0.0076 - acc: 0.9985 - val_loss: 1.0063 - val_acc: 0.7385\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 343s - loss: 0.0034 - acc: 0.9992 - val_loss: 1.0395 - val_acc: 0.7367\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 344s - loss: 0.0020 - acc: 0.9996 - val_loss: 0.9318 - val_acc: 0.7550\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 343s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.9079 - val_acc: 0.7831\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 343s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.9276 - val_acc: 0.7838\n"
     ]
    }
   ],
   "source": [
    "# vgg16 with BN,finetune layer 15  10 epoche\n",
    "run_a_model(VGG16, (224, 224), 15, preprocessing=None, model_name_option='_BN_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 311\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 506s - loss: 0.2324 - acc: 0.9389 - val_loss: 0.6160 - val_acc: 0.8082\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 489s - loss: 0.0385 - acc: 0.9894 - val_loss: 0.8902 - val_acc: 0.7666\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 489s - loss: 0.0316 - acc: 0.9911 - val_loss: 0.8109 - val_acc: 0.8302\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 489s - loss: 0.0157 - acc: 0.9951 - val_loss: 0.6184 - val_acc: 0.8552\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 489s - loss: 0.0148 - acc: 0.9956 - val_loss: 1.3184 - val_acc: 0.7166\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 497s - loss: 0.0075 - acc: 0.9975 - val_loss: 0.9058 - val_acc: 0.7954\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 489s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.8289 - val_acc: 0.8216\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 489s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.8779 - val_acc: 0.8277\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 489s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.8738 - val_acc: 0.8357\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 489s - loss: 0.0026 - acc: 0.9994 - val_loss: 0.9565 - val_acc: 0.8265\n"
     ]
    }
   ],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 180, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_180')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 311\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 472s - loss: 0.2829 - acc: 0.9258 - val_loss: 0.8754 - val_acc: 0.7502\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 464s - loss: 0.0513 - acc: 0.9857 - val_loss: 0.5692 - val_acc: 0.8180\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 464s - loss: 0.0228 - acc: 0.9929 - val_loss: 1.3623 - val_acc: 0.7434\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 464s - loss: 0.0163 - acc: 0.9950 - val_loss: 0.8817 - val_acc: 0.8088\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 464s - loss: 0.0156 - acc: 0.9952 - val_loss: 0.3770 - val_acc: 0.9023\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 473s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.4018 - val_acc: 0.8821\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 464s - loss: 0.0043 - acc: 0.9987 - val_loss: 0.5527 - val_acc: 0.8503\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 465s - loss: 0.0027 - acc: 0.9992 - val_loss: 0.5721 - val_acc: 0.8509\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 465s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.6307 - val_acc: 0.8454\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 465s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.6199 - val_acc: 0.8448\n"
     ]
    }
   ],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 200, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优秀模型继续优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 19\n",
      "Found 20787 images belonging to 10 classes.\n",
      "Found 1637 images belonging to 10 classes.\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 341s - loss: 0.2312 - acc: 0.9318 - val_loss: 6.6858 - val_acc: 0.3800\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 341s - loss: 0.0453 - acc: 0.9873 - val_loss: 3.3952 - val_acc: 0.5987\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 341s - loss: 0.0191 - acc: 0.9943 - val_loss: 3.3759 - val_acc: 0.6109\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 341s - loss: 0.0149 - acc: 0.9953 - val_loss: 3.0096 - val_acc: 0.5589\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 341s - loss: 0.0128 - acc: 0.9964 - val_loss: 2.6761 - val_acc: 0.5993\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 341s - loss: 0.0092 - acc: 0.9969 - val_loss: 3.4047 - val_acc: 0.4918\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 341s - loss: 0.0117 - acc: 0.9962 - val_loss: 3.7892 - val_acc: 0.5492\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 341s - loss: 0.0055 - acc: 0.9985 - val_loss: 2.8811 - val_acc: 0.6005\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 341s - loss: 0.0084 - acc: 0.9975 - val_loss: 2.2557 - val_acc: 0.6756\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 341s - loss: 0.0065 - acc: 0.9983 - val_loss: 2.5946 - val_acc: 0.6854\n",
      "Epoch 1/10\n",
      "163/163 [==============================] - 342s - loss: 8.8858e-04 - acc: 0.9999 - val_loss: 1.7507 - val_acc: 0.7410\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 341s - loss: 9.5102e-04 - acc: 0.9998 - val_loss: 1.7962 - val_acc: 0.7282\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 341s - loss: 6.6248e-04 - acc: 0.9999 - val_loss: 1.6051 - val_acc: 0.7392\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 341s - loss: 6.3125e-04 - acc: 0.9999 - val_loss: 1.5972 - val_acc: 0.7239\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 341s - loss: 4.5951e-04 - acc: 1.0000 - val_loss: 1.6436 - val_acc: 0.7245\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 341s - loss: 2.8845e-04 - acc: 1.0000 - val_loss: 1.6015 - val_acc: 0.7227\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 341s - loss: 4.1598e-04 - acc: 0.9999 - val_loss: 1.6156 - val_acc: 0.7221\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 341s - loss: 2.2808e-04 - acc: 1.0000 - val_loss: 1.5971 - val_acc: 0.7257\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 341s - loss: 3.5228e-04 - acc: 1.0000 - val_loss: 1.5541 - val_acc: 0.7373\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 341s - loss: 2.4054e-04 - acc: 1.0000 - val_loss: 1.5450 - val_acc: 0.7367\n"
     ]
    }
   ],
   "source": [
    "# VGG16 5 fine tune layer 5 more epoche\n",
    "run_a_model(VGG16, (224, 224), 15, model_name_option='_BN_15_20epoche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 19\n",
      "Found 20864 images belonging to 10 classes.\n",
      "Found 1560 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 385s - loss: 0.2480 - acc: 0.9271 - val_loss: 1.7392 - val_acc: 0.5641\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 341s - loss: 0.0405 - acc: 0.9893 - val_loss: 2.3495 - val_acc: 0.6333\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 341s - loss: 0.0246 - acc: 0.9930 - val_loss: 1.7060 - val_acc: 0.6218\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 341s - loss: 0.0138 - acc: 0.9964 - val_loss: 1.2859 - val_acc: 0.7218\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 341s - loss: 0.0073 - acc: 0.9977 - val_loss: 2.6110 - val_acc: 0.5994\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 342s - loss: 0.0079 - acc: 0.9974 - val_loss: 0.4301 - val_acc: 0.8827\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 340s - loss: 0.0078 - acc: 0.9978 - val_loss: 0.2986 - val_acc: 0.9282\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 340s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.3472 - val_acc: 0.9122\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 340s - loss: 0.0068 - acc: 0.9981 - val_loss: 0.3553 - val_acc: 0.9103\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 339s - loss: 0.0064 - acc: 0.9978 - val_loss: 0.3451 - val_acc: 0.9135\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 342s - loss: 0.0053 - acc: 0.9988 - val_loss: 0.3824 - val_acc: 0.9083\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 340s - loss: 0.0051 - acc: 0.9989 - val_loss: 0.3498 - val_acc: 0.9212\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 340s - loss: 0.0058 - acc: 0.9986 - val_loss: 0.3832 - val_acc: 0.9115\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 340s - loss: 0.0066 - acc: 0.9981 - val_loss: 0.3532 - val_acc: 0.9109\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 340s - loss: 0.0055 - acc: 0.9987 - val_loss: 0.3289 - val_acc: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7ff419310550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_a_model(VGG16, (224, 224), 15, model_name_option='_BN_15_SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 19\n",
      "Found 20864 images belonging to 10 classes.\n",
      "Found 1560 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 603s - loss: 0.2486 - acc: 0.9283 - val_loss: 2.4089 - val_acc: 0.5801\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 337s - loss: 0.0393 - acc: 0.9889 - val_loss: 1.0648 - val_acc: 0.6955\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 337s - loss: 0.0212 - acc: 0.9943 - val_loss: 1.8312 - val_acc: 0.5647\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 337s - loss: 0.0141 - acc: 0.9958 - val_loss: 1.6120 - val_acc: 0.6782\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 337s - loss: 0.0099 - acc: 0.9969 - val_loss: 1.5405 - val_acc: 0.7071\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 338s - loss: 0.0067 - acc: 0.9982 - val_loss: 0.4048 - val_acc: 0.8929\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 336s - loss: 0.0057 - acc: 0.9987 - val_loss: 0.3907 - val_acc: 0.8942\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 336s - loss: 0.0058 - acc: 0.9987 - val_loss: 0.3780 - val_acc: 0.8917\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 336s - loss: 0.0053 - acc: 0.9986 - val_loss: 0.3577 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 336s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.3837 - val_acc: 0.8949\n",
      "Epoch 1/10\n",
      "164/164 [==============================] - 338s - loss: 0.0051 - acc: 0.9989 - val_loss: 0.4063 - val_acc: 0.8872\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 336s - loss: 0.0042 - acc: 0.9991 - val_loss: 0.4172 - val_acc: 0.8923\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 336s - loss: 0.0050 - acc: 0.9990 - val_loss: 0.3804 - val_acc: 0.8904\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 336s - loss: 0.0065 - acc: 0.9983 - val_loss: 0.3833 - val_acc: 0.8955\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 336s - loss: 0.0059 - acc: 0.9983 - val_loss: 0.3957 - val_acc: 0.8897\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 336s - loss: 0.0056 - acc: 0.9984 - val_loss: 0.3936 - val_acc: 0.8897\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 336s - loss: 0.0050 - acc: 0.9990 - val_loss: 0.3615 - val_acc: 0.8981\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 336s - loss: 0.0053 - acc: 0.9989 - val_loss: 0.3773 - val_acc: 0.8917\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 336s - loss: 0.0060 - acc: 0.9987 - val_loss: 0.3875 - val_acc: 0.8923\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 336s - loss: 0.0057 - acc: 0.9985 - val_loss: 0.3603 - val_acc: 0.8994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f19caa4e810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_a_model(VGG16, (224, 224), 15, model_name_option='_BN_15_SGD_20e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 175\n",
      "Found 20864 images belonging to 10 classes.\n",
      "Found 1560 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 283s - loss: 0.2960 - acc: 0.9125 - val_loss: 1.3964 - val_acc: 0.6301\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 268s - loss: 0.0623 - acc: 0.9800 - val_loss: 1.7165 - val_acc: 0.5949\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 270s - loss: 0.0395 - acc: 0.9875 - val_loss: 1.8411 - val_acc: 0.6558\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 270s - loss: 0.0365 - acc: 0.9892 - val_loss: 1.5533 - val_acc: 0.6891\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 269s - loss: 0.0328 - acc: 0.9897 - val_loss: 0.9221 - val_acc: 0.7506\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 283s - loss: 0.0212 - acc: 0.9933 - val_loss: 0.9448 - val_acc: 0.7359\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 269s - loss: 0.0197 - acc: 0.9935 - val_loss: 0.9816 - val_acc: 0.7346\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 269s - loss: 0.0195 - acc: 0.9939 - val_loss: 0.9315 - val_acc: 0.7404\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 269s - loss: 0.0207 - acc: 0.9932 - val_loss: 0.9580 - val_acc: 0.7378\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 269s - loss: 0.0188 - acc: 0.9938 - val_loss: 0.9225 - val_acc: 0.7423\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 284s - loss: 0.0166 - acc: 0.9943 - val_loss: 0.9807 - val_acc: 0.7385\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 267s - loss: 0.0171 - acc: 0.9943 - val_loss: 0.9633 - val_acc: 0.7378\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 268s - loss: 0.0206 - acc: 0.9935 - val_loss: 0.9276 - val_acc: 0.7436\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 269s - loss: 0.0176 - acc: 0.9937 - val_loss: 0.9033 - val_acc: 0.7423\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 268s - loss: 0.0164 - acc: 0.9950 - val_loss: 0.9026 - val_acc: 0.7487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f197058c390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_a_model(ResNet50, (224, 224), 160, model_name_option='_BN_160_SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n",
      "623/623 [==============================] - 859s   \n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('models/model_ResNet50_BN_160_SGD.h5')\n",
    "y_predictions, test_id = make_predictions(model, (224, 224), 128)\n",
    "create_submission(y_predictions, test_id, tag=\"Resnet_160_SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 200, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_200_SGD')\n",
    "# finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n",
      "623/623 [==============================] - 1283s  \n"
     ]
    }
   ],
   "source": [
    "model = load_model('models/model_InceptionV3_BN_200_SGD.h5')\n",
    "y_predictions, test_id = make_predictions(model, (299, 299), 128, preprocessing=inception_v3.preprocess_input)\n",
    "create_submission(y_predictions, test_id, tag=\"Inception_200_SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total layer count 311\n",
      "Found 20864 images belonging to 10 classes.\n",
      "Found 1560 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "134/164 [=======================>......] - ETA: 89s - loss: 0.2937 - acc: 0.9253"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a52c4bd5f509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_a_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m170\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_BN_170_SGD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/model_InceptionV3_BN_170_SGD.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Inception_170_SGD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0ba24a5d5f71>\u001b[0m in \u001b[0;36mrun_a_model\u001b[0;34m(BASE_MODEL, input_shape, fine_tune_layer, preprocessing, model_name_option)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         validation_steps=steps_valid_sample)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlnd-dl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 170, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_170_SGD')\n",
    "model = load_model('models/model_InceptionV3_BN_170_SGD.h5')\n",
    "y_predictions, test_id = make_predictions(model, (299, 299), 128, preprocessing=inception_v3.preprocess_input)\n",
    "create_submission(y_predictions, test_id, tag=\"Inception_170_SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_a_model(Xception, (299, 299), 135, preprocessing=xception.preprocess_input, model_name_option='_BN_135_SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_a_model(InceptionV3, (299, 299), 180, preprocessing=inception_v3.preprocess_input, model_name_option='_BN_180_SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('models/model_VGG16_BN_15_SGD_20e.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img(path, image_shape):\n",
    "    img = cv2.imread(path, 0)\n",
    "    resized = cv2.resize(img, image_shape)\n",
    "    return resized\n",
    "\n",
    "def KNN(input_data, predictions, N=5, coef = [0.05, 0.1, 0.15, 0.2, 0.5]):\n",
    "    new_predictions_list = []\n",
    "    for i in tqdm(range(len(input_data))):\n",
    "        distance = np.square(np.subtract(input_data, input_data[i]))\n",
    "        distance = np.sqrt(np.sum(distance, axis=(1, 2)))\n",
    "        N_min_index = np.argsort(distance)[::-1][-N:]\n",
    "        \n",
    "        pred = [ predictions[N_min_index[j]] * coef[j] for j in range(N)]\n",
    "        pred = np.sum(pred, axis=0)\n",
    "        new_predictions_list.append(pred)\n",
    "                \n",
    "    return np.array(new_predictions_list)\n",
    "\n",
    "def make_predictions(MODEL, image_size, batch_size, preprocessing=None):\n",
    "    gen = ImageDataGenerator(preprocessing_function=preprocessing)\n",
    "    path_test_data = 'dataset/to_prediction'\n",
    "    test_generator = gen.flow_from_directory(path_test_data,  image_size, shuffle=False, \n",
    "                                             batch_size=batch_size, class_mode='categorical')\n",
    "    y_predictions = MODEL.predict_generator(test_generator,  steps=test_generator.samples//batch_size+1,  verbose=1)\n",
    "    # y_predictions = y_predictions.clip(min=0.005, max=0.995)\n",
    "    \n",
    "    test_id = list()\n",
    "    for i, file_name in enumerate(test_generator.filenames):\n",
    "        flbase = os.path.basename(file_name)\n",
    "        test_id.append(flbase)       \n",
    "    \n",
    "    return y_predictions, test_id\n",
    "\n",
    "def create_submission(predictions, test_id, tag = \"\"):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3',\n",
    "                                                 'c4', 'c5', 'c6', 'c7',\n",
    "                                                 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    if not os.path.isdir('subm'):\n",
    "        os.mkdir('subm')\n",
    "    suffix = str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('subm', 'submission_' + tag + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n",
      "623/623 [==============================] - 1756s  \n"
     ]
    }
   ],
   "source": [
    "y_predictions, test_id = make_predictions(model, (224, 224), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_predictions = y_predictions.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(y_predictions, test_id, tag=\"VGG16_SGD_20e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlnd-dl]",
   "language": "python",
   "name": "conda-env-mlnd-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
